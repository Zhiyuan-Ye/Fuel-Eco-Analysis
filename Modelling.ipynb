{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0eaaf2b8b3b82e37dd1161a33309e3ee8ad90dee8c759a3e17ba62f81edc7c209",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons.optimizers.RectifiedAdam as RAdam\n",
    "from tensorflow_addons.Lookahead import Lookahead\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# import dtale\n",
    "# import seaborn as sns\n",
    "# dtale.show(data_df, ignore_duplicate=True)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons.optimizers.RectifiedAdam'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-384718937bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRectifiedAdam\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mRAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLookahead\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLookahead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons.optimizers.RectifiedAdam'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Data Exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DataInit():\n",
    "    def __init__(self):\n",
    "        self.path = './vehicles.csv'\n",
    "\n",
    "    def get_df(self):\n",
    "        return pd.read_csv(self.path)\n",
    "\n",
    "    # Create a func for question 1 \"Which manufacturer produces the most fuel efficient fleet of cars?\"\n",
    "    # Assumption is that higher combined MPG, more efficient the vehicles are.\n",
    "    # def rank_manufacturers_by_fuel_efficiency(self, df):\n",
    "    #     df = df[['make', 'comb08', 'combA08', 'atvType']]\n",
    "\n",
    "    #     def fuel_efficiency_per_manufacturer(df):\n",
    "    #         # Here Oliver makes an assumption that dual fuel vehicle only uses fuelType2 to be fuel efficient.\n",
    "    #         df['comb_mpg'] = df.apply(lambda x: x['comb08'] if x['combA08'] == 0 else x['combA08'], axis=1)\n",
    "    #         avg_barrel = df['comb_mpg'].mean()\n",
    "    #         return avg_barrel\n",
    "\n",
    "    #     series = df.groupby('make').apply(fuel_efficiency_per_manufacturer)\n",
    "    #     series = series.sort_values(ascending=False)\n",
    "    #     return series"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Export descriptions of all columns for examination\n",
    "data = DataInit()\n",
    "data_df = data.get_df()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data_df.describe(include='all')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data_df.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data_df.nunique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Relationship Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correlation = data_df.corr() \n",
    "correlation['UCity'].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# correlation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(100, 30), dpi=50)\n",
    "cor_plt = sns.heatmap(correlation[['UCity']], xticklabels=['UCity'], yticklabels=correlation.columns, annot=True, ax=ax1) # heatmap只是用颜色表示了数值而已，装逼用的\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cor_plt.figure.savefig('Q4_corr_heatmap.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Numeric features normalization\n",
    "def zscore_normalization(series):\n",
    "    scaler = StandardScaler()\n",
    "    transformed_num = scaler.fit_transform(np.expand_dims(series, axis=1))\n",
    "    transformed_num = np.squeeze(transformed_num, axis=1)\n",
    "    return transformed_num\n",
    "\n",
    "\n",
    "def minmax_normalization(series):\n",
    "    scaler = MinMaxScaler()\n",
    "    transformed_num = scaler.fit_transform(np.expand_dims(series, axis=1))\n",
    "    transformed_num = np.squeeze(transformed_num, axis=1)\n",
    "    return transformed_num\n",
    "\n",
    "# Fill missing values\n",
    "def fill_missing_for_numeric_cols(x):\n",
    "    mean = x.mean()\n",
    "    x = x.fillna(mean)\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tokenizer(inputs): \n",
    "    id2idx = {id: index for index, id in enumerate(inputs)}\n",
    "    idx2id = {index: id for index, id in enumerate(inputs)}\n",
    "    return id2idx, idx2id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_trainable_data(df, numeric_cols):\n",
    "    # Normalize numeric cols\n",
    "    df[numeric_cols[:-1]] = df[numeric_cols[:-1]].apply(zscore_normalization) \n",
    "    df[numeric_cols[-1]] = df[numeric_cols[-1]].transform(minmax_normalization) \n",
    "    # fill in NaN for numeric cols\n",
    "    df[numeric_cols] = df[numeric_cols].apply(fill_missing_for_numeric_cols) \n",
    "    # fill in NaN for atvType col \n",
    "    df = df.fillna(value={'atvType': 'Petrol'})\n",
    "\n",
    "    atvtype2idx, idx2atvtype = tokenizer(df['atvType'].unique()) \n",
    "    engid2idx, idx2engid = tokenizer(df['engId'].unique())\n",
    "    make2idx, idx2make = tokenizer(df['make'].unique())\n",
    "    drive2idx, idx2drive = tokenizer(df['drive'].unique())\n",
    "\n",
    "    # replace categorical cols (ids) with index\n",
    "    df['atvType'] = df['atvType'].replace(atvtype2idx) \n",
    "    df['engId'].replace(engid2idx, inplace=True) \n",
    "    df['make'].replace(make2idx, inplace=True)\n",
    "    df['drive'].replace(drive2idx, inplace=True)\n",
    "\n",
    "    # get training data for RegressionEmbed model\n",
    "    x_all_RE = df.values[:, :-1] # \n",
    "    y_all_RE = df.values[:, -1]\n",
    "    # train/test/valid split \n",
    "    x_train_all_RE, x_test_RE, y_train_all_RE, y_test_RE = train_test_split(\n",
    "        x_all_RE, y_all_RE, train_size=0.8, random_state=42)\n",
    "    x_train_RE, x_valid_RE, y_train_RE, y_valid_RE = train_test_split(\n",
    "        x_train_all_RE, y_train_all_RE, train_size=0.8, random_state=42)\n",
    "\n",
    "    # get training data for LinerRegression\n",
    "    x_all_num_lr = df.values[:, :5]\n",
    "    x_all_cat_lr = df.values[:, 5:9]\n",
    "    y_all_lr = df.values[:, -1]\n",
    "\n",
    "    # One-Hot for categorical cols\n",
    "    one_hot_encoder = OneHotEncoder(drop='first', categories='auto')\n",
    "    x_all_cat_lr = one_hot_encoder.fit_transform(x_all_cat_lr).toarray()\n",
    "    x_all_lr = np.hstack((x_all_num_lr, x_all_cat_lr))\n",
    "    # train/test/valid split \n",
    "    x_train_all_lr, x_test_lr, y_train_all_lr, y_test_lr = train_test_split(\n",
    "        x_all_lr, y_all_lr, train_size=0.8, random_state=42)\n",
    "    x_train_lr, x_valid_lr, y_train_lr, y_valid_lr = train_test_split(\n",
    "        x_train_all_lr, y_train_all_lr, train_size=0.8, random_state=42)\n",
    "\n",
    "    return x_train_RE, y_train_RE, x_test_RE, y_test_RE, x_valid_RE, y_valid_RE, \\\n",
    "           atvtype2idx, idx2atvtype, engid2idx, idx2engid, make2idx, idx2make, drive2idx, idx2drive, \\\n",
    "           x_train_lr, y_train_lr, x_test_lr, y_test_lr, x_valid_lr, y_valid_lr\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rmse(y_actual, y_pred):\n",
    "    return np.sqrt(np.mean((y_actual - y_pred) ** 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_embeddings_dataframe(model, layer_name, idx_dic):\n",
    "    embedding_learnt = model.get_layer(name=layer_name).get_weights()[0]\n",
    "    df = pd.DataFrame(embedding_learnt)\n",
    "    df = df.rename(index=idx_dic)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_weights_dataframe(model, layer_names):\n",
    "    df = pd.DataFrame()\n",
    "    index_dict = {index: name for index, name in enumerate(layer_names)}\n",
    "    for idx, layer_name in enumerate(layer_names):\n",
    "        if idx == 0:\n",
    "            weights = model.get_layer(name=layer_name).get_weights()[0]\n",
    "            df = pd.DataFrame(weights.T)\n",
    "        else:\n",
    "            weights = model.get_layer(name=layer_name).get_weights()[0]\n",
    "            df = pd.concat([df, pd.DataFrame(weights.T)])\n",
    "            index_dict[idx] = layer_name\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.rename(index=index_dict)\n",
    "    return df, index_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class RegressionWithEmbed(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(RegressionWithEmbed, self).__init__()\n",
    "\n",
    "        self.numeric_weights = keras.layers.Dense(1, activation='relu', name='numericWeights')\n",
    "        self.atvtype_embedding = keras.layers.Embedding(atvtype_count, EMBEDDING_DIM,\n",
    "                                                        embeddings_initializer='uniform',\n",
    "                                                        embeddings_regularizer=keras.regularizers.l2(\n",
    "                                                            l=REGULIZATION_RATE),\n",
    "                                                        name='atvtypeEmbed')\n",
    "        self.atvtype_weights = keras.layers.Dense(1, activation='relu', name='atvtypeWeights')\n",
    "\n",
    "        self.engid_embedding = keras.layers.Embedding(engid_count, EMBEDDING_DIM,\n",
    "                                                      embeddings_initializer='uniform',\n",
    "                                                      embeddings_regularizer=keras.regularizers.l2(\n",
    "                                                          l=REGULIZATION_RATE),\n",
    "                                                      name='engidEmbed')\n",
    "        self.engid_weights = keras.layers.Dense(1, activation='relu', name='engidWeights')\n",
    "\n",
    "        self.make_embedding = keras.layers.Embedding(make_count, EMBEDDING_DIM,\n",
    "                                                     embeddings_initializer='uniform',\n",
    "                                                     embeddings_regularizer=keras.regularizers.l2(\n",
    "                                                         l=REGULIZATION_RATE),\n",
    "                                                     name='makeEmbed')\n",
    "        self.make_weights = keras.layers.Dense(1, activation='relu', name='makeWeights')\n",
    "\n",
    "        self.drive_embedding = keras.layers.Embedding(drive_count, EMBEDDING_DIM,\n",
    "                                                      embeddings_initializer='uniform',\n",
    "                                                      embeddings_regularizer=keras.regularizers.l2(\n",
    "                                                          l=REGULIZATION_RATE),\n",
    "                                                      name='driveEmbed')\n",
    "        self.drive_weights = keras.layers.Dense(1, activation='relu', name='driveWeights')\n",
    "\n",
    "    def call(self, x):\n",
    "        numeric_output = self.numeric_weights(x[0])\n",
    "        atvtype_output = self.atvtype_weights(self.atvtype_embedding(x[1]))\n",
    "        engid_output = self.engid_weights(self.engid_embedding(x[2]))\n",
    "        make_output = self.make_weights(self.make_embedding(x[3]))\n",
    "        drive_output = self.drive_weights(self.drive_embedding(x[4]))\n",
    "\n",
    "        output = keras.layers.Add()([numeric_output,\n",
    "                                     atvtype_output, engid_output, make_output, drive_output])\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# \"Build a model to predict city mpg (variable “UCity” in column BG).\"\n",
    "# UCity is unadjusted city MPG for fuelType1\n",
    "\n",
    "# STEP 1: Extract data from cvs and manually pick out features to use.\n",
    "NUMERIC_COLS = ['barrels08',  # annual petroleum consumption in barrels for fuelType1 (1)\n",
    "                'city08',  # city MPG for fuelType1 (2), (11)\n",
    "                'displ',  # engine displacement in liters\n",
    "                'highway08',  # highway MPG for fuelType1 (2), (11)\n",
    "                'year'\n",
    "                ]\n",
    "\n",
    "CATEGORICAL_COLS = ['atvType',\n",
    "                    'engId',\n",
    "                    'make',\n",
    "                    'drive'  # drive axle type\n",
    "                    ]\n",
    "\n",
    "TARGET = ['UCity']\n",
    "\n",
    "data = DataInit()\n",
    "data_df = data.get_df()\n",
    "data_df = data_df[NUMERIC_COLS + CATEGORICAL_COLS + TARGET] \n",
    "\n",
    "# Explore features\n",
    "print(data_df[NUMERIC_COLS].describe())\n",
    "print()\n",
    "for feature in CATEGORICAL_COLS:\n",
    "    print(\"{} has {} unique values\".format(feature, len(data_df[feature].unique())))\n",
    "\n",
    "# STEP 2: Get training data for RegressionWithEmbed and LinearRegression\n",
    "#         Because features \"make\" and \"engID\" have too many values (135/2661), one-hot encoding of LinearRegression might be problematic.\n",
    "#         So RegressionWithEmbed is used, with LinearRegression as baseline.\n",
    "x_train_RE, y_train_RE, x_test_RE, y_test_RE, x_valid_RE, y_valid_RE, \\\n",
    "atvtype2idx, idx2atvtype, engid2idx, idx2engid, make2idx, idx2make, drive2idx, idx2drive, \\\n",
    "x_train_lr, y_train_lr, x_test_lr, y_test_lr, x_valid_lr, y_valid_lr = get_trainable_data(data_df, numeric_cols=NUMERIC_COLS)\n",
    "\n",
    "# STEP 3: Build and train RegressionWithEmbed\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 10000\n",
    "EMBEDDING_DIM = 6\n",
    "LEARNING_RATE = 0.001\n",
    "REGULIZATION_RATE = 0.00015\n",
    "\n",
    "atvtype_count = len(atvtype2idx)\n",
    "engid_count = len(engid2idx)\n",
    "make_count = len(make2idx)\n",
    "drive_count = len(drive2idx)\n",
    "\n",
    "# RwE model initialization\n",
    "RegressionWithEmbed = RegressionWithEmbed()\n",
    "\n",
    "# optimizer and loss function and callbacks\n",
    "opt = Lookahead(RAdam(LEARNING_RATE))\n",
    "RegressionWithEmbed.compile(loss='mean_squared_error', optimizer=opt)\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3)]\n",
    "\n",
    "# train the model\n",
    "RegressionWithEmbed_history = RegressionWithEmbed.fit(\n",
    "    (x_train_RE[:, :5], x_train_RE[:, 5], x_train_RE[:, 6], x_train_RE[:, 7], x_train_RE[:, 8]), y_train_RE,\n",
    "    validation_data=((x_valid_RE[:, :5], x_valid_RE[:, 5], x_valid_RE[:, 6], x_valid_RE[:, 7], x_valid_RE[:, 8]), y_valid_RE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    "    )\n",
    "\n",
    "# plot learning curve\n",
    "plot_learning_curves(RegressionWithEmbed_history)\n",
    "# evaluate the model\n",
    "x = RegressionWithEmbed.evaluate(\n",
    "    (x_test_RE[:, :5], x_test_RE[:, 5], x_test_RE[:, 6], x_test_RE[:, 7], x_test_RE[:, 8]), y_test_RE,\n",
    "    batch_size=BATCH_SIZE)\n",
    "print(x)\n",
    "\n",
    "# STEP 4: Train LR\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train_lr, y_train_lr)\n",
    "\n",
    "# STEP 5: Compare RwE and LR\n",
    "# LR prediction\n",
    "Y_pred_lr = lr.predict(x_test_lr)\n",
    "# RwE prediction\n",
    "Y_pred_RE = RegressionWithEmbed.predict(\n",
    "    (x_test_RE[:, :5], x_test_RE[:, 5], x_test_RE[:, 6], x_test_RE[:, 7], x_test_RE[:, 8]))\n",
    "Y_pred_RE = np.squeeze(Y_pred_RE)\n",
    "\n",
    "test_dataset_results = pd.DataFrame(np.vstack((y_test_lr, Y_pred_lr, Y_pred_RE)).T, columns=['actual', 'lr', 'RE'])\n",
    "\n",
    "print(test_dataset_results.describe())\n",
    "print()\n",
    "\n",
    "test_dataset_results.to_csv('test_dataset_results.csv')\n",
    "\n",
    "print(\"lr's R2 is {}\".format(r2_score(y_test_lr, Y_pred_lr)))\n",
    "print(\"RE's R2 is {}\".format(r2_score(y_test_RE, Y_pred_RE)))\n",
    "print(\"lr's rmse is {}\".format(rmse(y_test_lr, Y_pred_lr)))\n",
    "print(\"RE's rmse is {}\".format(rmse(y_test_RE, Y_pred_RE)))\n",
    "\n",
    "'''As shown above, because some one-hot encoding features have extremely large weights, lr prediction fails at predicting some un-seen type of test data'''\n",
    "'''But RwW model converts categorical features into short fixed-length embeddings, hence it doesn't have above problem.'''\n",
    "\n",
    "# Check LR's performance after filtering out extremely wrong cases.\n",
    "print('LR has {} extreme predictions out of {} test samples'.format(\n",
    "    len(test_dataset_results[test_dataset_results['lr'] > 10000]) + len(test_dataset_results[test_dataset_results['lr'] < -10000]), len(test_dataset_results)))\n",
    "print()\n",
    "filtered_test_dataset_results = test_dataset_results[\n",
    "    ~((test_dataset_results['lr'] > 10000) | (test_dataset_results['lr'] < -10000))]\n",
    "\n",
    "print(\"After filter out extremely wrong predictions of lr, lr's R2 is {}\".format(\n",
    "    r2_score(filtered_test_dataset_results['lr'].values, filtered_test_dataset_results['actual'].values)))\n",
    "print(\"After filter out extremely wrong predictions of lr, lr's rmse is {}\".format(\n",
    "    rmse(filtered_test_dataset_results['lr'].values, filtered_test_dataset_results['actual'].values)))\n",
    "\n",
    "# Evaluate the weights of LR\n",
    "np.savetxt(\"lr_coef_.csv\", lr.coef_, delimiter=\",\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the weights of RwE\n",
    "numeric_layer_name = ['numericWeights']\n",
    "categorical_layer_names = ['atvtypeWeights', 'engidWeights', 'makeWeights', 'driveWeights']\n",
    "\n",
    "numeric_weights, _ = get_weights_dataframe(RegressionWithEmbed, numeric_layer_name)\n",
    "categorical_weights, _ = get_weights_dataframe(RegressionWithEmbed, categorical_layer_names)\n",
    "\n",
    "numeric_weights.to_csv('RwE_numeric_weights.csv')\n",
    "categorical_weights.to_csv('RwE_categorical_weights.csv')\n",
    "\n",
    "print(numeric_layer_name)\n",
    "print()\n",
    "print(categorical_weights)\n",
    "\n",
    "# Export the embeddings matrix for all categorical weights\n",
    "atvtype_embedding_df= get_embeddings_dataframe(RegressionWithEmbed, 'atvtypeEmbed', idx2atvtype)\n",
    "engid_embedding_df = get_embeddings_dataframe(RegressionWithEmbed, 'engidEmbed', idx2engid)\n",
    "make_embedding_df = get_embeddings_dataframe(RegressionWithEmbed, 'makeEmbed', idx2make)\n",
    "drive_embedding_df = get_embeddings_dataframe(RegressionWithEmbed, 'driveEmbed', idx2drive)\n",
    "\n",
    "atvtype_embedding_df.to_csv('atvtype_embedding.csv')\n",
    "engid_embedding_df.to_csv('engid_embedding.csv')\n",
    "make_embedding_df.to_csv('make_embedding.csv')\n",
    "drive_embedding_df.to_csv('drive_embedding.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}